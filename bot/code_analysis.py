from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
import torch
from chardet import detect
import logging
import os
from datetime import datetime
import re
import warnings

# –§–∏–ª—å—Ç—Ä –≤—Å–µ—Ö –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)

class CodeAnalyzer:
    def __init__(self, model_name, rag_db_path):
        self.logger = logging.getLogger(__name__)
        self.model_name = model_name
        self.generator = None  # –ö–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–∞–∫ –∫–æ–Ω–≤–µ–π–µ—Ä
            self._init_generator()
            
            # –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π RAG (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            self.rag_db = self._init_rag_db(rag_db_path)
            
        except Exception as e:
            self.logger.critical(f"Init error: {str(e)}")
            raise

    def _init_generator(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∫–∞–∫ –∫–æ–Ω–≤–µ–π–µ—Ä–∞"""
        device = 0 if torch.cuda.is_available() else -1
        self.generator = pipeline(
            "text-generation",
            model=self.model_name,
            tokenizer=self.tokenizer,
            device=device,
            torch_dtype=torch.float16 if device == 0 else torch.float32,
            trust_remote_code=True
        )

    def _init_rag_db(self, path):
        if not path or not os.path.exists(path):
            return None
            
        try:
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-mpnet-base-v2",
                model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
            )
            return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)
        except Exception as e:
            self.logger.warning(f"RAG init failed: {str(e)}")
            return None

    def analyze_file(self, file_path: str) -> str:
        try:
            if self._should_skip(file_path):
                return "‚ÑπÔ∏è File skipped (binary/system)"
                
            code = self._read_file_content(file_path)
            if not code:
                return "‚ö†Ô∏è Failed to read file"
                
            analysis = self.fast_generate_analysis(file_path, code)
            filename = os.path.basename(file_path)
            return self._postprocess_analysis(analysis, filename)
            
        except Exception as e:
            self.logger.error(f"Analysis failed: {str(e)}")
            return f"‚ö†Ô∏è Analysis error: {str(e)}"


    def _should_skip(self, file_path):
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º—ã—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π
        skip_exts = {
            '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.zip', '.rar',
            '.exe', '.dll', '.so', '.bin', '.pdf', '.ttf', '.woff', '.woff2',
            '.eot', '.otf', '.mp3', '.wav', '.mp4', '.avi', '.mov', '.doc',
            '.docx', '.xls', '.xlsx', '.pyc', '.gitignore', '.md', '.ini', '.txt',
            '.json', '.yaml', '.yml', '.log', '.lock', '.toml', '.cfg', '.conf'
        }
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º .git —Ñ–∞–π–ª—ã –∏ –±–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–∞–π–ª—ã
        if '/.git/' in file_path.replace('\\', '/'):
            return True
            
        ext = os.path.splitext(file_path)[1].lower()
        if ext in skip_exts:
            return True
            
        return False

    def _read_file_content(self, file_path):
        try:
            with open(file_path, 'rb') as f:
                raw = f.read()
                encoding = detect(raw)['encoding'] or 'utf-8'
                return raw.decode(encoding, errors='replace')
        except Exception as e:
            self.logger.error(f"Error reading file {file_path}: {str(e)}")
            return None

    def fast_generate_analysis(self, file_path, code):
        """–ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω–≤–µ–π–µ—Ä–∞"""
        max_tokens = 3000
        encoded = self.tokenizer.encode(code)
        if len(encoded) > max_tokens:
            code = self.tokenizer.decode(
                encoded[:max_tokens],
                skip_special_tokens=True
            ) + "\n\n... [–∫–æ–¥ —É—Å–µ—á–µ–Ω –∏–∑-–∑–∞ –¥–ª–∏–Ω—ã]"
        
        # –ü—Ä–∏–º–µ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ (few-shot learning)
        example = (
            "[example.py]\n"
            "(–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å)\n"
            "[—Å—Ç—Ä–æ—á–∫–∞: password = '12345']\n"
            "[–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: password = getpass.getpass()]\n\n"
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
        prompt = (
            "–¢—ã - –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–¥–∞. –ù–∞–π–¥–∏ –ø—Ä–æ–±–ª–µ–º—ã –∏ –≤—ã–≤–µ–¥–∏ –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n"
            "1. [–Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞]\n"
            "2. (—Ç–∏–ø –ø—Ä–æ–±–ª–µ–º—ã)\n"
            "3. [—Å—Ç—Ä–æ—á–∫–∞: –ø—Ä–æ–±–ª–µ–º–Ω–∞—è —Å—Ç—Ä–æ–∫–∞]\n"
            "4. [–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞]\n"
            "–ü—Ä–∏–º–µ—Ä:\n" + example +
            "–ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º –Ω–µ—Ç - –¢–û–õ–¨–ö–û '–ü—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ'\n\n"
            f"–§–∞–π–ª: {os.path.basename(file_path)}\n–ö–æ–¥:\n{code}"
        )
        
        try:
            result = self.generator(
                prompt,  # –ü–µ—Ä–µ–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É, –∞ –Ω–µ —Å–ø–∏—Å–æ–∫
                max_new_tokens=500,
                temperature=0.01,
                top_p=0.95,
                do_sample=True,
                num_return_sequences=1,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.eos_token_id,
            )
            
            response = result[0]['generated_text'].strip()
            # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
            return response.replace(prompt, "", 1).strip()
            
        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"

    def _postprocess_analysis(self, analysis, filename):
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É"""
        # –®–∞–±–ª–æ–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–æ–ª–Ω—ã—Ö –±–ª–æ–∫–æ–≤
        block_pattern = r"\[([^\]]+)\]\s*\(([^)]+)\)\s*\[—Å—Ç—Ä–æ—á–∫–∞:\s*([^\n]+)\]\s*\[–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ:\s*([^\n]+)\]"
        blocks = re.findall(block_pattern, analysis)
        
        if blocks:
            formatted = []
            for block in blocks:
                # –û—á–∏—Å—Ç–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
                file_part = block[0].strip() or filename
                problem_type = block[1].strip()
                problem_line = block[2].strip()
                fix_line = block[3].strip()
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–ª–æ–∫–∞
                formatted.append(
                    f"[{file_part}]\n"
                    f"({problem_type})\n"
                    f"[—Å—Ç—Ä–æ—á–∫–∞: {problem_line}]\n"
                    f"[–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {fix_line}]"
                )
            return "\n\n".join(formatted)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–±–ª–µ–º
        if "–ü—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ" in analysis:
            return "–ü—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ"
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        clean_analysis = re.sub(
            r"(–¢—ã - –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–¥–∞|–ù–∞–π–¥–∏ –ø—Ä–æ–±–ª–µ–º—ã|–ü—Ä–∏–º–µ—Ä:|–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞:|–ñ–ï–°–¢–ö–ò–ï –ü–†–ê–í–ò–õ–ê:).*", 
            "", 
            analysis,
            flags=re.DOTALL
        ).strip()
        
        if clean_analysis:
            return clean_analysis
        
        return f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞"

        

    def generate_report(self, file_paths: list) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        final_report = "# –û—Ç—á—ë—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–¥–∞\n\n"
        valid_files = 0
        
        for path in file_paths:
            if not self._should_skip(path):
                clean_path = os.path.normpath(path).replace("\\", "/")
                analysis = self.analyze_file(path)
                
                if not analysis.strip():
                    analysis = "ü§∑ –ê–Ω–∞–ª–∏–∑ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –í–æ–∑–º–æ–∂–Ω–æ, —Ñ–∞–π–ª —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–µ–Ω –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."
                
                if analysis.startswith("‚ÑπÔ∏è") or analysis.startswith("‚ö†Ô∏è"):
                    final_report += f"## –§–∞–π–ª: `{clean_path}`\n{analysis}\n\n"
                    valid_files += 1
                    continue
                
                # –ü—Ä—è–º–æ–π –≤—ã–≤–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                final_report += f"## –§–∞–π–ª: `{clean_path}`\n{analysis}\n\n"
                valid_files += 1
                    
        if valid_files == 0:
            final_report += "‚ö†Ô∏è –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n"
            final_report += "–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n"
            final_report += "- –í—Å–µ —Ñ–∞–π–ª—ã –ø—Ä–æ–ø—É—â–µ–Ω—ã –∫–∞–∫ –±–∏–Ω–∞—Ä–Ω—ã–µ/—Å–∏—Å—Ç–µ–º–Ω—ã–µ\n"
            final_report += "- –û—à–∏–±–∫–∏ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤\n"
            final_report += "- –í —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –Ω–µ—Ç —Ñ–∞–π–ª–æ–≤ –∫–æ–¥–∞\n"
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        reports_dir = "data/reports"
        os.makedirs(reports_dir, exist_ok=True)
        
        final_path = os.path.join(reports_dir, f"report_{timestamp}.md")
        with open(final_path, "w", encoding="utf-8") as f:
            f.write(final_report)
            
        return final_path