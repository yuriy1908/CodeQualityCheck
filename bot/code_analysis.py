from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
import torch
from chardet import detect
import logging
import os
from datetime import datetime
import re
import warnings

# –§–∏–ª—å—Ç—Ä –≤—Å–µ—Ö –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)

class CodeAnalyzer:
    def __init__(self, model_name, rag_db_path):
        self.logger = logging.getLogger(__name__)
        self.model_name = model_name
        self.generator = None  # –ö–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–∞–∫ –∫–æ–Ω–≤–µ–π–µ—Ä
            self._init_generator()
            
            # –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π RAG (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            self.rag_db = self._init_rag_db(rag_db_path)
            
        except Exception as e:
            self.logger.critical(f"Init error: {str(e)}")
            raise

    def _init_generator(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∫–∞–∫ –∫–æ–Ω–≤–µ–π–µ—Ä–∞"""
        device = 0 if torch.cuda.is_available() else -1
        self.generator = pipeline(
            "text-generation",
            model=self.model_name,
            tokenizer=self.tokenizer,
            device=device,
            torch_dtype=torch.float16 if device == 0 else torch.float32,
            trust_remote_code=True
        )

    def _init_rag_db(self, path):
        if not path or not os.path.exists(path):
            return None
            
        try:
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-mpnet-base-v2",
                model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
            )
            return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)
        except Exception as e:
            self.logger.warning(f"RAG init failed: {str(e)}")
            return None

    def analyze_file(self, file_path: str) -> str:
        try:
            if self._should_skip(file_path):
                return "‚ÑπÔ∏è File skipped (binary/system)"
                
            code = self._read_file_content(file_path)
            if not code:
                return "‚ö†Ô∏è Failed to read file"
                
            analysis = self.fast_generate_analysis(file_path, code)
            filename = os.path.basename(file_path)
            return self._postprocess_analysis(analysis, filename)
            
        except Exception as e:
            self.logger.error(f"Analysis failed: {str(e)}")
            return f"‚ö†Ô∏è Analysis error: {str(e)}"


    def _should_skip(self, file_path):
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º—ã—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π
        skip_exts = {
            '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.zip', '.rar',
            '.exe', '.dll', '.so', '.bin', '.pdf', '.ttf', '.woff', '.woff2',
            '.eot', '.otf', '.mp3', '.wav', '.mp4', '.avi', '.mov', '.doc',
            '.docx', '.xls', '.xlsx', '.pyc', '.gitignore', '.md', '.ini', '.txt',
            '.json', '.yaml', '.yml', '.log', '.lock', '.toml', '.cfg', '.conf'
        }
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º .git —Ñ–∞–π–ª—ã –∏ –±–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–∞–π–ª—ã
        if '/.git/' in file_path.replace('\\', '/'):
            return True
            
        ext = os.path.splitext(file_path)[1].lower()
        if ext in skip_exts:
            return True
            
        return False

    def _read_file_content(self, file_path):
        try:
            with open(file_path, 'rb') as f:
                raw = f.read()
                encoding = detect(raw)['encoding'] or 'utf-8'
                return raw.decode(encoding, errors='replace')
        except Exception as e:
            self.logger.error(f"Error reading file {file_path}: {str(e)}")
            return None

    def fast_generate_analysis(self, file_path, code):
        """–ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω–≤–µ–π–µ—Ä–∞"""
        max_tokens = 3000
        encoded = self.tokenizer.encode(code)
        if len(encoded) > max_tokens:
            code = self.tokenizer.decode(
                encoded[:max_tokens],
                skip_special_tokens=True
            ) + "\n\n... [–∫–æ–¥ —É—Å–µ—á–µ–Ω –∏–∑-–∑–∞ –¥–ª–∏–Ω—ã]"
        
        filename = os.path.basename(file_path)
        
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
        prompt = (
            f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–¥ —Ñ–∞–π–ª–∞ {filename} –Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, —Å—Ç–∏–ª—è, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –æ—à–∏–±–æ–∫.\n"
            f"–ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º –Ω–µ—Ç, –≤—ã–≤–µ–¥–∏ '–ü—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ'.\n"
            f"–ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã –µ—Å—Ç—å, –≤—ã–≤–µ–¥–∏ –∏—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n"
            f"[—Å—Ç—Ä–æ—á–∫–∞: –ø—Ä–æ–±–ª–µ–º–Ω–∞—è —Å—Ç—Ä–æ–∫–∞]\n"
            f"[–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞]\n\n"
            f"–ö–æ–¥:\n{code}\n\n"
            f"–ê–Ω–∞–ª–∏–∑:"
        )
        
        try:
            result = self.generator(
                prompt,
                max_new_tokens=400,
                temperature=0.1,
                top_p=0.9,
                do_sample=True,
                num_return_sequences=1,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.eos_token_id,
            )
            
            response = result[0]['generated_text'].strip()
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ "–ê–Ω–∞–ª–∏–∑:"
            if "–ê–Ω–∞–ª–∏–∑:" in response:
                response = response.split("–ê–Ω–∞–ª–∏–∑:", 1)[-1].strip()
            
            return response
            
        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"

    def _postprocess_analysis(self, analysis, filename):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–π –≤—ã–≤–æ–¥ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        self.logger.debug(f"Raw analysis for {filename}:\n{analysis}")
        
        # –®–∞–±–ª–æ–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º
        problem_pattern = r"\[—Å—Ç—Ä–æ—á–∫–∞:\s*(.+?)\]\s*\[–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ:\s*(.+?)\]"
        problems = re.findall(problem_pattern, analysis, re.DOTALL)
        
        if problems:
            formatted = []
            for problem in problems:
                problem_line = problem[0].strip()
                fix_line = problem[1].strip()
                
                # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                problem_line = re.sub(r'\s+', ' ', problem_line)
                fix_line = re.sub(r'\s+', ' ', fix_line)
                
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Å—Ç—Ä–æ–∫
                problem_line = problem_line[:200]
                fix_line = fix_line[:200]
                
                formatted.append(
                    f"[—Å—Ç—Ä–æ—á–∫–∞: {problem_line}]\n"
                    f"[–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {fix_line}]"
                )
            
            return f"[{filename}]\n" + "\n\n".join(formatted)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–±–ª–µ–º
        if re.search(r"–ü—Ä–æ–±–ª–µ–º\s+–Ω–µ\s+–æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ", analysis, re.IGNORECASE):
            return f"[{filename}]\n–ü—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö
        if "–û—à–∏–±–∫–∞" in analysis or "‚ö†Ô∏è" in analysis:
            return f"[{filename}]\n{analysis}"
        
        # –ï—Å–ª–∏ –≤—ã–≤–æ–¥ –Ω–µ –ø—É—Å—Ç–æ–π, –Ω–æ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ñ–æ—Ä–º–∞—Ç—É
        if analysis.strip():
            return f"[{filename}]\n{analysis.strip()}"
        
        return f"[{filename}]\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞"


    def generate_report(self, file_paths: list) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        final_report = "# –û—Ç—á—ë—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–¥–∞\n\n"
        valid_files = 0
        
        for path in file_paths:
            if not self._should_skip(path):
                clean_path = os.path.normpath(path).replace("\\", "/")
                analysis = self.analyze_file(path)
                
                if not analysis.strip():
                    analysis = "ü§∑ –ê–Ω–∞–ª–∏–∑ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –í–æ–∑–º–æ–∂–Ω–æ, —Ñ–∞–π–ª —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–µ–Ω –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."
                
                if analysis.startswith("‚ÑπÔ∏è") or analysis.startswith("‚ö†Ô∏è"):
                    final_report += f"## –§–∞–π–ª: `{clean_path}`\n{analysis}\n\n"
                    valid_files += 1
                    continue
                
                # –ü—Ä—è–º–æ–π –≤—ã–≤–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                final_report += f"## –§–∞–π–ª: `{clean_path}`\n{analysis}\n\n"
                valid_files += 1
                    
        if valid_files == 0:
            final_report += "‚ö†Ô∏è –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n"
            final_report += "–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n"
            final_report += "- –í—Å–µ —Ñ–∞–π–ª—ã –ø—Ä–æ–ø—É—â–µ–Ω—ã –∫–∞–∫ –±–∏–Ω–∞—Ä–Ω—ã–µ/—Å–∏—Å—Ç–µ–º–Ω—ã–µ\n"
            final_report += "- –û—à–∏–±–∫–∏ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤\n"
            final_report += "- –í —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –Ω–µ—Ç —Ñ–∞–π–ª–æ–≤ –∫–æ–¥–∞\n"
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        reports_dir = "data/reports"
        os.makedirs(reports_dir, exist_ok=True)
        
        final_path = os.path.join(reports_dir, f"report_{timestamp}.md")
        with open(final_path, "w", encoding="utf-8") as f:
            f.write(final_report)
            
        return final_path